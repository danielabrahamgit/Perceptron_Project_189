{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Binary Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gain a deeper understanding of Machine Learning and Neural Networks, we are going to have to start out by learning about Perceptrons.  In this notebook we will be building our own Perceptron class by hand, examining its performance on different datasets, and learning how to use prebuilt python libraries to implement this classifier.   \n",
    "\n",
    "\n",
    "\n",
    "Perceptrons are the building blocks of Neural Networks, and while this may sound complicated, their function is well defined by simple Linear Algebra concepts that you are already very familiar with.  As with Neural Networks in general, Perceptrons can be understood through an analogy to biology.  Our brain is composed of Neurons; each Neuron has many dendrites branching out to recieve signals, a cell body that process these inputs, and a single Axon which sends out information to other Neurons.  While a single Neuron cannot accomplish much on its own, combining a whole network of them allows for the incedible functionality that we get from our brain.  An Artificial Neural Network functions in much the same way, with powerful emergent properties coming out of the combination of simple building blocks.  In this analogy, the Perceptron acts as a single Neuron.  And while in later weeks we will examine how Neural Networks are formed by linking these together, this week we will focus on the structure of a single Perceptron and the problems it is able to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "A Perceptron is known as a Binary Linear Classifier.  This means it is a good model to use so long as we have a set of linearly separable data that we want to put into two different categories.  While later in this homework you will look into the different python toolkits that allow us to work with Perceptrons, we will start off by building one by hand.  Perceptrons are relatively simple to construct, and doing so should give you a deeper understanding of how and when to use the prebuilt libraries.\n",
    "\n",
    "![title](Materials/perceptronModel.png)\n",
    "\n",
    "Above is a picture of a Perceptron. From here the components should be pretty easy to see.  We have an array of inputs, their corresponding weights, and an activation function (in the above model this activation function is more specifically a step function).  Make sure this picture makes sense to you, because now we will be building out a Perceptron class.  <br>Specifically, we want our Perceptron class to be able to do the following:\n",
    "* Take a weighted sum\n",
    "* Compute an activation function\n",
    "* Train our weights\n",
    "* Predict a classification\n",
    "\n",
    "It is okay if all of those bullet points don't make sense to you right now.  We suggest scrolling down to read over the perceptron class and get a sense for what code you need to fill in.  But the specifics are covered in the next few sections, going over precisely what needs to be implemented and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Weighted Sum\n",
    "\n",
    "The first step in building a Perceptron is being able to calculate the weighted sum. The summation of weighted inputs should draw your mind to a familiar concept.  Note how this changes when you want process multiple inputs at once. Specifically for a $d$ dimensioned datapoint, the function we are coding is as follows:\n",
    "$$weightedSum=\\sum_{i=1}^d x_i*w_i$$\n",
    "\n",
    "Fill in **Part1** of the weightedSum function.  This function should be able to handle multiple inputs at once, where $X$ is passed in as a $R^{nxd}$ dimensioned matrix where $n$ is the number of datapoints and $d$ is their dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Activation Function\n",
    "\n",
    "Next we will take a look at the activation function.  An activation function is a non-linear function that in our case maps a value to a binary output.  There are a lot of different activation functions that can be used, but the one we use for Perceptron is very simple:\n",
    "\n",
    "$$ f(x) = \n",
    "        \\begin{cases}\n",
    "        1, x \\geq 0 \\\\\n",
    "        -1, x < 0\n",
    "        \\end{cases} \n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass in our weighted sum to this activation function, we are using information about a point of data to assign it to one of two categories, -1 or 1.  The binary part of Binary Linear Classifier should now make sense. We have started a function called activation below, fill in the code for **Part 2** so that the activation performs as it is defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Prediction\n",
    "\n",
    "A Perceptron should be able to take an input and predict what class it belongs to, -1 or 1.  With what we have currently, you should be able to implement this logic.  Write the code for **Part 3** in the prediction function of the Perceptron class.  Make sure to utalize the functions you have already written. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Train Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be able to get a good prediction for our data if we don't have the correct weights.  Really, the main \n",
    "point of Perceptron is to determine the weight on each input that will allow us to achieve accurate classification of our data.  We do this by initializing our weights to zero and then using the Perceptron update formula.  To perform an update, we will loop through every datapoint and if that point (with true value $y^*_i$) is misclassified, change the weights using:\n",
    "\n",
    "$${w}_{new} = {w}_{old} + r * y^*_i * {x}_i$$\n",
    "\n",
    "Implement this update formula in **Part 4** of the train function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \"\"\"\n",
    "    Perceptron class for Binary Linear Classification.\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "        \n",
    "    def weightedSum(inputs,weights):\n",
    "        \"\"\"\n",
    "        Write the code for calculating the Perceptrons weighted sum.\n",
    "        inputs - nxd dimensional matrix\n",
    "\n",
    "        return - nx1 column vector of weighted sums\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        #Use self.weights for the weights\n",
    "        #TODO: PART 1\n",
    "        #Start \n",
    "        \n",
    "        #End\n",
    "\n",
    "        return result\n",
    "\n",
    "    def activation(x):\n",
    "        \"\"\"\n",
    "        Write the code for the Perceptron activation function.\n",
    "        x - nx1 column vector\n",
    "\n",
    "        return - nx1 column vector of 1's and 0's\n",
    "        \"\"\"\n",
    "        result = None\n",
    "\n",
    "        ##TODO: PART 2\n",
    "        #Start\n",
    "        \n",
    "        #End\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def predict(self,inputs):\n",
    "        \"\"\"\n",
    "        Calculates predicted class for each input\n",
    "        \n",
    "        inputs - n x d matrix\n",
    "        \n",
    "        return - n x 1 column vector\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        #TODO: Part 3\n",
    "        #Start\n",
    "        \n",
    "        #End\n",
    "        return pred\n",
    "    \n",
    "    def train(self, inputs, classes):\n",
    "        \"\"\"\n",
    "        Update self.weights for better classification.\n",
    "        \n",
    "        inputs - n x d matrix\n",
    "        classes - n x 1 column vector\n",
    "        \"\"\"\n",
    "        num_samples = inputs.shape[0]\n",
    "        dim = inputs.shape[1]\n",
    "        self.weights = np.zeros((dim,1))\n",
    "        r = .1\n",
    "\n",
    "        stop = 10000\n",
    "        converged = True\n",
    "        for i in range(stop):\n",
    "            #TOD0: Part 4\n",
    "            #Start\n",
    "            \n",
    "            #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this point you may have two questions.  What is r? And when do we stop updating?  r is our learning rate. it scales how much our weights change each update. This is know as a hyperparameter, and while you don't need to understand what that means right now, just know it is a value we can change to optimize our performace. In this notebook just set r=.1. \n",
    "And as for when to stop updating, we can see from the algorithm that this is done once we have no incorrect classifications in a single update loop. However, for data that is not linearly separable, this case will never be reached and we will get stuck in an infinite loop. Therefore it is good practice to put a break condition into the code such as a max length of time or number of iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data\n",
    "With our Perceptron class defined, we can now take a look at how it performs on actual data.  Imagine you are playing a little game with your friends.  Sally has two types of candy; both look identical but one is incredibly spicy.  She lays these candies out on the table according to a linear pattern, and you are trying to avoid the spicy ones while getting the tasty ones.  Sally promises that she will keep the pattern the same for everyone and lets you go last.  \n",
    "\n",
    "You think that you can train a Perceptron to uncover this pattern, so you pay close attention to where all the tasty and spicy candies were for your other friends.  Run the cell below to generate the observed locations of the candy and their corresponding flavors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(num, min_error):\n",
    "    \"\"\"\n",
    "    Generates previously seen candy locations.\n",
    "    \n",
    "    return - (data, classes)\n",
    "        data: num x 2 Matrix of [x, y] positions\n",
    "        classes: num x 1 column vector of 1s (first num/2) and -1s (last num/2)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    classes = []\n",
    "    split = int(num/2)\n",
    "    for i in range(split):\n",
    "        x = randrange(-10,10)\n",
    "        y = x + 12\n",
    "        y += randrange(min_error,12)\n",
    "        data.append([x,y])\n",
    "        classes.append([1])\n",
    "    for i in range(split):\n",
    "        x = randrange(-10,10)\n",
    "        y = x + 12\n",
    "        y += randrange(-12,-min_error)\n",
    "        data.append([x,y])\n",
    "        classes.append([-1])\n",
    "    data = np.array(data)\n",
    "    classes = np.array(classes)\n",
    "    return (data, classes)\n",
    "\n",
    "candy_locations, candy_flavors = generateData(60,4)\n",
    "plt.plot(candy_locations[:30,0], candy_locations[:30,1], 'bo', candy_locations[30:,0], candy_locations[30:,1], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Unseen Data\n",
    "It is almost your turn to play the game, but you want to test how well your Perceptron does before you put your tastebuds on the line.  John is going directly before you, so you try to predict the outcome of his game.  **Finish the getAccuracy function** below that calculates how accurately your perceptron predicts the labels of new data. \n",
    "\n",
    "After writing that code, run the cell to see how well your perceptron does on John's data and **comment on the results you see**. Does it do a good job predicting?  Why or why not? How does the visualization help explain what is going on?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_locations, john_flavors = generateData(20,4) #You should compare your perceptron's result to john_flavors\n",
    "candy_perceptron = Perceptron() #Train this perceptron\n",
    "\n",
    "def getAccuracy(perceptron, X, Y, newX, newY):\n",
    "    \"\"\"\n",
    "    Calculates how accurately perceptron predicts the newY for newX after being trained on X and Y\n",
    "    \n",
    "    perceptron - the perceptron to train and use for predictions\n",
    "    X - X data for training (candy_locations) \n",
    "    Y - Labels for training (candy_flavors) \n",
    "    newX - newly aquired data for prediction (john_locations)\n",
    "    newY - labels of new data for determining accuracy of prediction (john_flavors)\n",
    "    \"\"\"\n",
    "    Accuracy = 0\n",
    "    #TODO\n",
    "    #Start\n",
    "\n",
    "    #End\n",
    "    \n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "\n",
    "#Calculating how well your perceptron predicts the flavor of john's candy\n",
    "getAccuracy(candy_perceptron, candy_locations, candy_flavors, john_locations, john_flavors)\n",
    "\n",
    "#Visualizing your results\n",
    "plt.plot(john_locations[:10,0], john_locations[:10,1], 'bo', john_locations[10:,0], john_locations[10:,1], 'ro')\n",
    "weights = candy_perceptron.weights\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, -(weights[0]/weights[1])*x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Your Comments Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Add Bias\n",
    "\n",
    "You may have noticed that your perceptron did not do a great job making predictions and separating the data.  In this section we will explore why this is.  If you look closely at the perceptron diagram at the start of this assignment, you should notice one key aspect that we have not included.  It is the constant term that is added into the weighted sum, this is what we call the bias.  To analyze why this is needed, lets take a look at the 2D example of a Perceptron.  If we have inputs $x_1$ and $x_2$ and weights $w_1$ and $w_2$, taking the weighted sum combined with our activation function gives us a boundary at:\n",
    "\n",
    "$$x_1*w_1 + x_2*w_2 = 0$$\n",
    "$$x_2 = -\\frac{w_1}{w_2}*x_1$$\n",
    "\n",
    "As you can see, this defines a line at the origin with a slope of $-\\frac{w_1}{w_2}$.  If you look back at the results from John's data, you should see that the line our perceptron generated is indeed going through the origin.  Clearly no such line will be good at separating our data, so we need to add a bias if we want a line with a different y-intercept.  When we append a constant to our input and give it a corresponding weight, our 2D example becomes:\n",
    "\n",
    "$$1*w_0 + x_1*w_1 + x_2*w_2 = 0$$\n",
    "$$x_2 = -\\frac{w_1}{w_2}*x_1 - \\frac{w_0}{w_2}$$\n",
    "\n",
    "As you can see, this defines a new line with the same slope as before, but a y-intercept now at $- \\frac{w_0}{w_2}$.  This extends to higher dimensions as well; a constant term should always be augmented to the input data. **Lets implement this step by filling in the addBias function below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBias(inputs):\n",
    "        \"Returns the inputs but with the bias added in\"\n",
    "        #TODO: Part 5\n",
    "        #Start\n",
    "        inputs = np.concatenate([np.ones((inputs.shape[0], 1)), inputs], axis=1)\n",
    "        #End\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Accuracy with Bias\n",
    "After failing at predicting John's candy flavors the first time, you have your fingers crossed that the added bias term will get your Perceptron working better.  **Fill in code below to calculate your accuracy on the augmented data and comment on the performance you see.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "getAccuracy(candy_perceptron, addBias(candy_locations), candy_flavors, addBias(john_locations), john_flavors)\n",
    "#End\n",
    "\n",
    "#Visualizing your results\n",
    "plt.plot(john_locations[:10,0], john_locations[:10,1], 'bo', john_locations[10:,0], john_locations[10:,1], 'ro')\n",
    "weights = candy_perceptron.weights\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, -(weights[1]/weights[2])*x - (weights[0]/weights[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Your Comments Here:\n",
    "Adding the bias in fixes our issues.  The Perceptron can now generate an appropriate boundary that exactly seperates our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Run\n",
    "It is finally your turn to play the game; armed with your perceptron you are feeling very confident in your chances of avoiding all of the spicy candy. Sally puts down the candy on the table following the same linear pattern as before.  You run your perceptron and eat all the candies that it predicts to be tasty.  Look at the output below and comment on the results you see.  **Did you end up eating any spicy candy? If so, what do you think went wrong? And come up with at least one thing you might be able to do to increase your accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sally_locations, sally_flavors = generateData(20,0) \n",
    "getAccuracy(candy_perceptron, addBias(candy_locations), candy_flavors, addBias(sally_locations), sally_flavors)\n",
    "#Visualizing you results\n",
    "plt.plot(sally_locations[:10,0], sally_locations[:10,1], 'bo', sally_locations[10:,0], sally_locations[10:,1], 'ro')\n",
    "weights = candy_perceptron.weights\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, -(weights[1]/weights[2])*x - (weights[0]/weights[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Your Comments Here:\n",
    "Unless you are very lucky, you probably noticied that the perceptron misclassified some of the candy.  Even after taking data from your friends' games, the Perceptron was not able to pick up on the exact linear pattern that Sally was following.  So when she laid out the candy for your, some of the pieces right near the boundary were labeled incorrectly.  There are a couple valid solutions, an easy one is to watch your friends play more games so you can gather more data.  Another less obvious solution that touches on topics learned later in this course would be to pick a boundary line exactly inbetween the two classes of data (if you scroll up you should see that our Perceptron's line isn't totally centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Perceptron Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this first part of the notebook has given you a better understanding of what a Perceptron is and how they are used.  After building your own Perceptron class you should not only understand why it is called a Binary Linear Classifier, but also see the importance of the bias term.  Finally, the last section should have made clear some of the shortcomings of Perceptron, especially when we are lacking data.  In the next part of this notebook we will be exploring how Perceptron can (and can't) be used on some more advanced problems.  You will also be introcuded to the prebuilt python libraries that you can use to implement Perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multiclass Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the notebook, we will cover an extension of the Binary Perceptron that was done above. Recall that for the Binary Perceptron, one requirement of the data was that there could only be two output classes. This meant that if we wanted to classify some input as more than just two outputs, we would have to look elsewhere for a solution. However, with some slight modifications to the Binary Perceptron, we are able to classify inputs with any amount of labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate studying the Multiclass Perceptron, let us revisit the candy example. Recall that Sally had two types of candy (spicy or sweet) and our job was to determine what type a candy was. Suppose that Sally now has three types of candy: spicy, sweet, and salty. Now instead of laying out the candies in some linear pattern, Sally decides to make it easier for us by putting candies of similar type near each other into clusters. Even though our goal still remains the same, we cannot use the binary perceptrons since there's more than two classes. We represent the candy type in code as labels 0, 1, or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiclass_data(c1, c2, c3, dis, size):\n",
    "    center1 = c1\n",
    "    center2 = c2\n",
    "    center3 = c3\n",
    "    distance = dis\n",
    "    size = size\n",
    "    data = []\n",
    "    classes = []\n",
    "    x1 = np.random.uniform(center1[0], center1[0] + distance, size=(size,))\n",
    "    y1 = np.random.normal(center1[1], distance, size=(size,)) \n",
    "    for i in range(size):\n",
    "        data.append([x1[i], y1[i]])\n",
    "        classes.append(0)\n",
    "\n",
    "    x2 = np.random.uniform(center2[0], center2[0] + distance, size=(size,))\n",
    "    y2 = np.random.normal(center2[1], distance, size=(size,)) \n",
    "    for i in range(size):\n",
    "        data.append([x2[i], y2[i]])\n",
    "        classes.append(1)\n",
    "\n",
    "    x3 = np.random.uniform(center3[0], center3[0] + distance, size=(size,))\n",
    "    y3 = np.random.normal(center3[1], distance, size=(size,)) \n",
    "    for i in range(size):\n",
    "        data.append([x3[i], y3[i]])\n",
    "        classes.append(2)\n",
    "        \n",
    "    plt.scatter(x1, y1)\n",
    "    plt.scatter(x2, y2)\n",
    "    plt.scatter(x3, y3)\n",
    "    plt.show()\n",
    "\n",
    "    return np.array(data), np.array(classes)\n",
    "    \n",
    "candy_data, candy_classes = generate_multiclass_data((-5, -5), (3, 20), (12, 5), 4, 40)\n",
    "candy_data = addBias(candy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the data, even though our binary perceptron will not work on this type of data, we should be able to classify the candy since they are in very distinct groups.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Modifying Weights\n",
    "The first modification we make to our perceptron is to the weights. In the binary case, we could get away with just using one weight vector because by taking the dot product of this vector and the input, we could use a threshold to classify into two cases. However, with more than two cases we cannot use this trick and thus need a weight vector for each possible class. Update the MultiClassPerceptron to account for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Modifying Classification\n",
    "Because we have more than one weight vector, we cannot classify by just taking a single dot product. Instead, to classify an input point, we compute the dot product of the point with every weight vector. The class that we predict the input as is then the class corresponding to the largest dot product. Mathematically, this is taking the argmax of the dot product of our input and every weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this mean for our activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our activation function now is simply the argmax of all of the weighted sums. \n",
    "![title](Materials/linear_multiclass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Modifying Update Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have multiple weight vectors, our update algorithm needs to be modified slightly. We will still only update the weights when a point is misclassified. However, instead of updating a single weight vector we will update all of them. If we incorrectly classify a point, we will update the correct weight by adding the input scaled by the learning rate, and update the other weights by subtracting that value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is our new update scheme really doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our new classification algorithm finds the largest dot product between the input and weights, if we incorrectly classify a point we want to increase this dot product for the correct class and decrease it for the other classes. Ultimately, we want the dot product with the correct class to be the largest out of all weight vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassPerceptron():\n",
    "    \"\"\"\n",
    "    Perceptron class for Multiclass Linear Classification.\n",
    "    \"\"\"\n",
    "        \n",
    "    def weightedSum(self, inputs, weight):\n",
    "        \"\"\"\n",
    "        Calculates the weighted sum of an input and some weight vector\n",
    "        \"\"\"\n",
    "        return inputs@weight\n",
    "\n",
    "    #TODO\n",
    "    #Start\n",
    "    def predict(self, training_point):\n",
    "        \"\"\"\n",
    "        Calculates predicted class for a input \n",
    "        \"\"\"\n",
    "        \n",
    "        argmax = 0\n",
    "        max_weight = self.weightedSum(training_point, self.weights[0])\n",
    "        for i, weight in enumerate(self.weights):\n",
    "            if max_weight < self.weightedSum(training_point, weight):\n",
    "                argmax = i\n",
    "                max_weight = self.weightedSum(training_point, weight)\n",
    "        return argmax\n",
    "                    \n",
    "    \n",
    "    def train(self, inputs, classes):\n",
    "        \"\"\"\n",
    "        Update self.weights for better classification.\n",
    "        \n",
    "        inputs - n x d matrix\n",
    "        classes - n x 1 column vector\n",
    "        \"\"\"\n",
    "        self.weights = []\n",
    "        num_samples = inputs.shape[0]\n",
    "        dim = inputs.shape[1]\n",
    "        for i in range(3):\n",
    "            self.weights.append(np.zeros((dim,1)))\n",
    "        r = .1\n",
    "        stop = 10000\n",
    "        converged = True\n",
    "        for i in range(stop):\n",
    "            for x in range(num_samples):\n",
    "                pred = self.predict(inputs[x])\n",
    "                if classes[x] != pred:\n",
    "                    converged = False\n",
    "                    modifier = r*inputs[x]\n",
    "                    modifier = modifier.reshape((dim,-1))\n",
    "                    for j, _ in enumerate(self.weights):\n",
    "                        if j != classes[x]:\n",
    "                            self.weights[j] -= modifier\n",
    "                        else:\n",
    "                            self.weights[j] += modifier\n",
    "            if converged:\n",
    "                break\n",
    "            converged = True\n",
    "        print(\"Converged:\", converged)\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Training Multiclass Perceptron on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test our newly built Multiclass Perceptron to classify Sally's candies. Using the **candy_data** and **candy_classes** variable that were generated above, train the Multiclass Perceptron and report the correct weight vectors for that set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "mcp = MulticlassPerceptron()\n",
    "mcp.train(candy_data, candy_classes)\n",
    "print(\"Trained Weight Vectors:\", mcp.weights)\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Testing Performance of Multiclass Perceptron on Unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to see how well our classifier performs on new candies that Sally puts out. With the new validation dataset generated below, write a function that prints the accuracy of our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Data\n",
    "new_candy_data, new_candy_classes = generate_multiclass_data((-6, -6), (4, 19), (11, 4), 5, 20)\n",
    "new_candy_data = addBias(new_candy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "def getMcpAccuracy(mcp, data, classes):\n",
    "    wrong = 0\n",
    "    for i, training_point in enumerate(data):\n",
    "        if (classes[i] != mcp.predict(training_point)):\n",
    "            #incorrectly classified\n",
    "            wrong += 1\n",
    "    print(\"Accuracy:\", 1-wrong/len(data))\n",
    "getMcpAccuracy(mcp, new_candy_data, new_candy_classes)\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did your classifier do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier should perform pretty well given that the new data generated has a very similar distribution to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Perceptron Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with some minor modifications we can make our binary perceptron work for the case of multiple classes. Still, the issues of the original binary perceptron remain: we can only work with linearly separable data, and we cannot say anything about the optimality of our weights. Keep these issues in mind in the coming weeks when we will be studying SVMs, Multilayer Perceptrons, and Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Speech Recognition With a Single Layer Perceptron\n",
    "\n",
    "We will be investigating a binary classification problem on audio data with two labels. The two labels are 'cat' and 'dog'. We will input raw audio data of humans saying the words 'cat' and 'dog'. We will then process this data and apply a few techniques to illustrate that perceptrons can be used to classify audio data.\n",
    "\n",
    "You will need the dataset of words, here are two ways to obtain it:\n",
    "\n",
    "**Method 1**\n",
    "The dataset that we will be using can be found at this link:\n",
    "https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html\n",
    "Downloading this dataset might take a few minutes. You will notice a folder with all kinds of folders, each for a different word. We are only interested in cat and dog. \n",
    "\n",
    "**Method 2**\n",
    "We have provided a zip file with all the necessary datasets you need. You can simply unzip it **in the same directory**\n",
    "\n",
    "Inside these respective cat and dog folders are raw .wav files. We will by using scipy to convert these raw wav files into discrete time signals. Let us first import all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is very important that you provide the relative file path to the dog and cat files. We will refer to dogs as 1 and cats as 0 in our binary classifcation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the data set containing dog audio samples\n",
    "path_1  = \"Materials/Word_Dataset/dog/\"\n",
    "#Path to the data set containing cat audio samples\n",
    "path_0  = \"Materials/Word_Dataset/cat/\"\n",
    "files_1 = os.listdir(path_1)\n",
    "files_0 = os.listdir(path_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define some usefull functions that will help us process the data. \n",
    "\n",
    "The first function is **numpy_fillna**. This function will simply take in a 2d numpy array where the row vectors are not the same length, and return a 2d numpy array where all the row vectors have the same length. To do this, we simply zero-pad the short row vectors/data vectors to match the length of the longer ones. Worry not, this does not change the relevent audio data at all!\n",
    "\n",
    "The second function is **get_train_test**. This function performs a few important steps, let us break it down:\n",
    "1. It will first use the argument N (total number of data points, 50% dog, 50% cat), and r_train (the ratio of training data to testing data) to calculate the number of training samples that we need. The big idea here is to take our data set and partition it into a training dataset and a testing dataset. The training dataset (can be something like 80% of all the data) will be used to actually train the perceptron. The testing dataset (whatever is leftover, 20%) will be used to acually test our perceptron on 'fresh' data.\n",
    "\n",
    "2. When accumulating the data from the .wav files, this function will trim the audio files nicely. It does this because alot of the times, the audio files have a very unessesary sequence of zeros before the initial useful audio data arrives. It will first find the time point where the audio hits 50% of the maximum value. Then, we instruct the function to call the start of the audio vector 'pre' units before this 50% time point. We can tune pre to untill the audio data appears to be alligned.\n",
    "\n",
    "3. It will also normalize all the audio vectors by removing their mean and making their variance = 1. The reason for this is to treat very loud audio the same as very soft audio. We have no preference over loud or soft audio when classifying. We subtract the mean in order to remove the offset in audio, which we also do not care about.\n",
    "\n",
    "4. Finally, the function will put all these normalized and trimmed audio vectors into a large 2d array. Now there is no gurantee that all the row vectors (data/audio vectors) in this 2d array will have the same length. This is where we use **numpy_fillna** to assert this property. Once this is done, we shuffle up the row vectors in order to make the data randomized with respect to the labels. A minor detail: The function will also return the samplerate of the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_fillna(data):\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(lens.max()) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=np.float64)#data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out\n",
    "\n",
    "def get_train_test(N, pre = 50, r_train = 0.5):\n",
    "    \n",
    "    N_train = int(r_train * N)\n",
    "    \n",
    "    X_1 = []\n",
    "    X_0 = []\n",
    "    \n",
    "    for i in range(N//2):\n",
    "        _, data_1 = wavfile.read(os.path.join(path_1, files_1[i]))\n",
    "        samplerate = _\n",
    "        data_1 = (data_1 - np.mean(data_1)) / np.std(data_1)\n",
    "        ind = 0\n",
    "        thresh = 0.5 * np.max(data_1)\n",
    "        while abs(data_1[ind]) < thresh:\n",
    "            ind += 1\n",
    "        ind -= pre\n",
    "        data_1 = data_1[ind:]\n",
    "        X_1.append(data_1)\n",
    "        \n",
    "        _, data_0 = wavfile.read(os.path.join(path_0, files_0[i]))\n",
    "        data_0 = (data_0 - np.mean(data_0)) / np.std(data_0)\n",
    "        ind = 0\n",
    "        thresh = 0.5 * np.max(data_0)\n",
    "        while abs(data_0[ind]) < thresh:\n",
    "            ind += 1\n",
    "        ind -= pre\n",
    "        data_0 = data_0[ind:]\n",
    "        X_0.append(data_0)\n",
    "        \n",
    "    X = numpy_fillna(np.asarray(X_0 + X_1))\n",
    "    y = np.append(np.zeros(N//2), np.ones(N//2)).reshape(-1, 1)\n",
    "    \n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(X)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(y)\n",
    "    \n",
    "    y = y.flatten()\n",
    "    \n",
    "    X_train = X[:N_train]\n",
    "    y_train = y[:N_train]\n",
    "    X_test  = X[N_train:]\n",
    "    y_test  = y[N_train:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, samplerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is nice and 'clean' let us define what we have mathematically. We will call the ith audio file from each file $\\vec{d_i}, \\vec{c_i} \\in R^d$ for the dog and cat respectively. The dimension $d$ of these vectors is much larger than the number of data points, $n$. $X \\in R^{n x d}, \\vec{y} \\in R^n$, the data matrix with corresponding labels will take the following form:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "\\vec{d_{21}}^T\\\\\n",
    "\\vec{d_{309}}^T\\\\\n",
    "\\vec{c_{19}}^T\\\\\n",
    "\\vec{d_{10}}^T\\\\\n",
    "\\vec{c_{111}}^T\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\n",
    "\\end{bmatrix} \\; \\; \\; \\;\\; \\;\\; \\; \\; \\; \\vec{y} = \\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If we chose an 80%, 20% split for train and test data, then $X_{train}$ will just be the first 80% rows vectors of $X$, and $X_{test}$ will be the remaining 20%. Similaraly, $\\vec{y_{train}}$ will be the first 80% of the enteries of $\\vec{y}$ and $\\vec{y_{test}}$ will be the remaining 20%. Or mathematically:\n",
    "\n",
    "\n",
    "## $$N_{train} = N * (0.8), N_{test} = N - N_{train}$$\n",
    "\n",
    "\n",
    "## $$X_{train} \\in R^{N_{train} x d}, X_{test} \\in R^{N_{test} x d}, y_{train} \\in R^{N_{train}}, y_{test} \\in R^{N_{test}}$$\n",
    "\n",
    "\n",
    "Now let us fetch the X and y training and testing pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, samplerate = get_train_test(N = 1500, pre = 500, r_train = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now we have our testing and training data, we are ready to use the perceptron! To do this, we can use the Sklearn's perceptron package. In the previous part of this jupyter notebook, we learned how a perceptron works and how to build one. For this part, we will use the imported perceptron module for simplicity. \n",
    "\n",
    "First thing we do is create our perceptron. There are many parameters we can pass into the Perceptron constructor, however the default parameters will be fine for this project. Here is a link if you are curious about Sklearn's Perceptron under the hood:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "\n",
    "## **(TODO) Using Sklearn's perceptron class, find the weights to fit X_train and y_train. Report the weights as a column vector. Make sure to name the column vector of weights $w$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Sklearn Perceptron code\n",
    "#TODO\n",
    "my_perceptron = Perceptron()\n",
    "my_perceptron.fit(X_train, y_train)\n",
    "#Make weights into a column vector\n",
    "w  = my_perceptron.coef_[0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need a way to test our classifier. Find a value for training_accuracy below which we define as the percentage of correctly classified training points. Recall that given some sample data, we can classify it by taking the inner product (dot product) of our weights with the input data. That quantity should come out to a scalar. We determine that if the scalar is positive, we assign a classification value of 1 and if it is negative we assign a classification value of 0. Or mathematically:\n",
    "$$y_{predicted} = S(\\vec{x_{sample}}^T \\vec{weights})$$\n",
    "\n",
    "If we have multiple input sample points in a matrix:\n",
    "$$X = \\begin{bmatrix}\n",
    "\\vec{x_{1}}^T\\\\\n",
    "\\vec{x_{2}}^T\\\\\n",
    "\\vec{x_{3}}^T\\\\\n",
    "\\vec{x_{4}}^T\\\\\n",
    "\\vec{x_{5}}^T\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\n",
    "\\end{bmatrix}, \\; \\; \\; \\; \\vec{y_{predicted}} = S(X \\vec{weights})$$\n",
    "\n",
    "Where $S(x)$ is $1$ if $x$ is positive and $0$ if $x$ is negative. If we have $S(\\vec{x})$ then we apply S to each element in the vector $\\vec{x}$ independently. \n",
    "\n",
    "## **(TODO) Find the percentage of correctly classified data points that the weights calculated earlier will predict on X_train in comparison to y_train. Store the accuracy into a variable named $training\\_accuracy$ (HINT: look into the Perceptron's .score function).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "training_accuracy = my_perceptron.score(X_train, y_train)\n",
    "#End\n",
    "print('Training Accuracy: {0:.2f}%'.format(training_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! It seems that our classifier has fit the training data very well. We will now do the same thing to the testing data:\n",
    "\n",
    "## **(TODO) Find the percentage of correctly classified data points that the weights calculated earlier will predict on X_test in comparison to y_test. Store the accuracy into a variable named $testing\\_accuracy$ (HINT: look into the Perceptron's .score function).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "testing_accuracy = my_perceptron.score(X_test, y_test)\n",
    "#End\n",
    "print('Testing Accuracy: {0:.2f}%'.format(testing_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good. What happened here? Well it seems that we have over-fit our data. We know this because we fit the training data extremely well, however the testing data gets missclassified almost all the time (recall that the worst binary classifer will have an accuracy of 50%). That is, our perceptron only knows how to fit what it has seen before. This is often the product of having too many parameters. Let us view just how many paramters we have in relation to the amount of training data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = len(w)\n",
    "num_data_points = len(X_train)\n",
    "print('Number of Parameters:', num_params)\n",
    "print('Number of Data Points:', num_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a ton of parameters. Perhaps it would be smart to perform PCA on this data in order to reduce its dimensionality (number of paramters) down to 2, just to see how distinguishable our test data is. **You do not need to understand PCA in order to follow the notebook. Just know that PCA converts our data matrix from $\\mathbb{R}^{n \\times d}$ to $\\mathbb{R}^{n \\times 2}$. That is, each data point will only have dimension 2. This will only be used to visualize our high dimension data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "num_data, dim = X_train_pca.shape\n",
    "print(\"Number of data points:\", num_data)\n",
    "print(\"Number of parameters/dimension of data:\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have reduced our data matrix to having the same number of data points, however now each data point only has dimension 2. Recall that X_train_pca still has our same number of data points, but each data point effectively only has an 'x' and a 'y' component. What we now want is the ability to plot each of these training data points.\n",
    "\n",
    "Let us build a function that takes in an input matrix and label array X and y respectively. The dimension of X will be N x 2 and y should be a 1 dimensional array of length N. The function will look one row at a time and plot the row of X as an ordered pair on a 2d plot. The color assigned to each label should depend on the value of y for that row. For example, let us say that we wish to plot the 5th row in our data matrix X. We can find the x and y component by looking at the first and second entry of the 5th row of X. Then, we can view the 5th entry in y. If the value is a 1, we can assign the color blue, and 0 gets the color red. Use the plt.scatter function to perform this task:\n",
    "\n",
    "## **(TODO) Impliment the $plot\\_data\\_2d$ function. This should plot label '1' data points as blue and label '0' data points as red** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_2d(X_2d, y_labels):\n",
    "    assert len(X_2d) == len(y_labels)\n",
    "    assert X_2d.shape[1] == 2\n",
    "    N = len(X_2d)\n",
    "    #TODO\n",
    "    #Start\n",
    "    for i in range(N):\n",
    "        row = X_2d[i]\n",
    "        if y_labels[i] == 1:\n",
    "            color = 'blue'\n",
    "        else: \n",
    "            color = 'red'\n",
    "        plt.scatter(row[0], row[1], c = color)\n",
    "    #End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented the function, let us plot the 2d projection of our data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_2d(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look very good. Recall that a perceptron is a linear classifer. However, in 2 dimensions, our data is not linearly seperable. \n",
    "\n",
    "**THE DFT PORTION IS COMPLETELY OPTIONAL TO UNDERSTAND. READ IF YOU ARE CURIOUS**\n",
    "\n",
    "At this point, we may feel like it is time to use other techniques to classify this data. However, 16b to the rescue, we have a very handy tool at our disposal. The discrete fourier transform, or DFT! Why would we even think of using this? Well, first let us ask, what is the best machine for classifying audio? It is us, humans that are the best at this task. This is partially due to the fact that the human ear canal performs a pseudo fourier transform.\n",
    "\n",
    "Taking the DFT of the audio data will provide us with information on how dominant certain frequencies are. This is important for speech recognition because that human audio only exists in a narrow frequency band (< 4kHz). So, if we take the DFT of our data and omit all frequencies above 6000 Hz (to be safe), we will essentially only be capturing the relevant informatin from the audio sample. This is a method of taking an extremeley long audio sample and reducing it down to its more fundemental components, which is what we wanted to do! Recall that our problem was that the vectors were too long/had too many parameters. By taking the DFT and omitting all frequencies above 6000Hz, we will be reducing the number of parameters significantly.\n",
    "\n",
    "In 16B we learned how to compute DFTs using the DFT matrix. In practice, we use the FFT or the Fast Fourier Transform. This will result in the same thing, however the FFT computes the DFT in $O(nlogn)$ time rather than $O(n^2)$ time with the 16B method. For the purposes of this notebook, don't worry about how the FFT works, **just know that the output is the same as if you computed the DFT via the DFT matrix**.\n",
    "\n",
    "So, let us go ahead and compute the 4096 point FFT (remember, the same input/output behavior of the DFT) of each raw audio data vector and construct a new matrix $X_w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#think of this as the resolution of our FFT\n",
    "NFFT=4096 \n",
    "\n",
    "#Compute fft for all data points\n",
    "Xw_train = None\n",
    "\n",
    "for arr in X_train:\n",
    "    if Xw_train is None:\n",
    "        Xw_train = np.fft.fft(arr, n = NFFT)\n",
    "    else:\n",
    "        Xw_train = np.vstack((Xw_train, np.fft.fft(arr, n = NFFT)))\n",
    "\n",
    "Xw_test = None\n",
    "\n",
    "for arr in X_test:\n",
    "    if Xw_test is None:\n",
    "        Xw_test = np.fft.fft(arr, n = NFFT)\n",
    "    else:\n",
    "        Xw_test = np.vstack((Xw_test, np.fft.fft(arr, n = NFFT)))\n",
    "\n",
    "#compute the 6khz cutoff index\n",
    "six_cutoff = int(NFFT * 6000 / samplerate)\n",
    "        \n",
    "#We only care about the magnitude of the complex numbers, hence np.abs\n",
    "Xw_train = np.abs(Xw_train[:, :six_cutoff])\n",
    "Xw_test = np.abs(Xw_test[:, :six_cutoff])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of what the data vectors now look like, let us plot the first data vector (now in frequency domain).\n",
    "\n",
    "## **(Optional) Choose any data point from X_train. Plot it as a time domain signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "plt.plot(X_train[4])\n",
    "plt.title('Time Signal Label 0')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(Optional) Choose the same row from before, but now grab the corresponding row of Xw_train. Plot it as a frequency domain signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "fVals=np.arange(start = 0,stop = six_cutoff)*samplerate/NFFT\n",
    "plt.plot(fVals,Xw_train[4])\n",
    "plt.title('One Sided FFT')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('|DFT Values|')\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(Optional) Comment on the differences between the frequency domain signal and the time domain signals:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency domain signal is able to capture seemingly relevant data with only 4000 samples while the time domain signal seems to have important values throughout 8000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have successfully converted all of our data into frequency domain! Let us do the same thing that we did before: Project the data back to 2 dimensions using PCA in order to plot the 'dog's in red and the 'cat's in blue. We will use our previously implimented funtion to plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "Xw_train_pca = pca.fit_transform(Xw_train)\n",
    "plot_data_2d(Xw_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we have linearly separable data! Now, we will train a perceptron on the FFT data rather than the raw data.\n",
    "\n",
    "## **(TODO) Use the same process from the previous parts to train a perceptron classifier but this time use the Xw_train data for training. Make sure to store the final weights into a variable called $w\\_fft$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Perceptron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d138a0bf9956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmy_perceptron_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmy_perceptron_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXw_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mw_fft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_perceptron_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Perceptron' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#Start\n",
    "my_perceptron_freq = Perceptron(tol=1e-3, random_state=0)\n",
    "my_perceptron_freq.fit(Xw_train, y_train)\n",
    "w_fft = my_perceptron_freq.coef_[0].reshape(-1, 1)\n",
    "#End\n",
    "print('Number of parameters/dimension for FFT:', len(w_fft))\n",
    "print('Number of parameters/dimension for Raw:', len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to reduce the dimensionality significantly and achieve linear separable data! Let us see how it performs:\n",
    "\n",
    "## **(TODO) Use the same process from the previous parts to report the testing accuracy and training accuracy for the FFT method. Remember to use Xw_train and Xw_test. Store the training and testing accuracies into variables named $training\\_accuracy$ and $testing\\_accuracy$ respectively.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Start\n",
    "training_accuracy = my_perceptron_freq.score(Xw_train, y_train)\n",
    "testing_accuracy = my_perceptron_freq.score(Xw_test, y_test)\n",
    "#End\n",
    "print('Training Accuracy FFT Method: {0:.2f}%'.format(training_accuracy * 100))\n",
    "print('Testing Accuracy FFT Method: {0:.2f}%'.format(testing_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have just classified a dog from a cat! We left the perceptron code to you and we handeled all of the data triming and pre processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(TODO) Comment below what you learned about perceptrons, audio classification, and linearly separable data.**\n",
    "\n",
    "We learned that perceptrons, which are natrurally linear classifiers, can also be use to classify non linear data. How did we accomplish this task? We used a non-linear featurization (changing raw audio data to frequency domain data). This was a very strong method of reducing the dimesnionality of our input data to something more managable and representative of the data. We also learned that something as simple as a single layer perceptron can be surprisingly powerful given the right data and the right choice of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
